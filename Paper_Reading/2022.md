# 1: 20220506
### Title: Convolutional Neural Networks Can Be Deceived by Visual Illusions
### Venue: CVPR 2019
这篇文章研究了视觉错觉（Visual Illusions）相关的内容，例如相同的颜色在不同的背景下，人类的视觉会错认为不同的颜色。实验发现CNN可以像人类视觉系统一样，也会有视觉错觉的现象。
# 2: 20220507
### Title: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks
### Venue: ICCV 2017
作者提出cyclegan来解决Unpaired Image-to-Image Translation问题，在训练GAN时，加入了Cycle Consistency Loss，模型不仅需要有能力translate X to Y，还要有能力translate Y to X。cyclegan被应用在多种任务中，有Object transfiguration，Season transfer，Photo generation from paintings等。
# 3: 20220509
### Title: Learning to Structure an Image with Few Colors
### Venue: CVPR 2020
本文研究了在严格的color约束下，图片如何保留更多的structure。提出ColorCNN，基于autoencoder，目标是image在极其小的颜色空间，尽可能地被classifier正确识别。
# 4: 20220509
### Title: Diverse Image-to-Image Translation via Disentangled Representations
### Venue: ECCV 2018
作者将image的解耦表达应用在Image-to-Image Translation，使其不再需要paired data做训练，同时增加了模型生成样本的多样性。提出DRIT方法。DRIT采用的仍然是GAN的结构，在其中加入了两个encoder，分别编码image的domain-invariant and domain-specific特征。
# 5: 20220509
### Title: Multimodal Unsupervised Image-to-Image Translation
### Venue: ECCV 2018
本文提出 Multimodal Unsupervised Image-to-image Translation (MUNIT) 框架来解决image-to-image translation任务中，转换的结果缺少多样性的问题。MUNIT将一张image解耦为content feature和style feature，使得方法可以实现example-guided image translation。
# 6: 20220510
### Title: Swapping Autoencoder for Deep Image Manipulation
### Venue: NIPS 2020
作者将image解耦为texture和structure，针对的任务是image manipulation。个人感觉与4和5做的工作非常相似。
# 7: 20220513
### Title: 绘画艺术图像的计算美学: 研究前沿与展望
### Venue: 自动化学报 2020
曾任国际实验美学协会主席的 Leder 将人类的审美行为建模为多层次的信息处理模型, 包含潜意识和主观意识两方面过程。潜意识过程包含对颜色、对比度、复杂性等底层信息的感知, 以及对个人经历和记忆的整合, 缺乏显式的信息输出, 难以被量化建模. 而主观意识过程包含显示分类、认知和评价三部分, 具有可被量化的中间结果或审美输出, 可以作为绘画图像计算美学的梳理参考。

本文将主观意识过程（分类、认知和评价），对应属性识别、内容理解和美学评价的计算美学问题，以这个角度展开每一部分的介绍。
# 8: 20220517
### Title: Improved Denoising Diffusion Probabilistic Models
### Venue: PMLR 2021
作者对DDPM做了一些改进，提高了采样速度，得到更好的log-likelihoods。
# 9: 20220519
### Title: Masked Autoencoders Are Scalable Vision Learners
### Venue: CVPR 2022
这篇文章将NLP任务的BERT模型应用于CV任务。采用encoder-decoder的架构，encoder的输入是image上一些随机的patch（把原图一些部分盖住），decoder的输入是encoder的输出潜编码和原图中被遮住的patch。这篇文章提出的是一个backbone，可以将其应用到别的下流任务中，如目标检测。
# 10: 20220528
### Title: ImageNet Classification with Deep Convolutional Neural Networks
### Venue: NIPS 2012
这是卷积神经网络的奠基之作，但现在看文章的写作有一些不美的地方，论文中的结论也有一些是不重要的。这个工作是在imagenet上做classification，在当年取得了非常好的效果，超过其余方法。惊奇的是，文章读起来很多名词依然是我们现在经常使用的，和读现在的paper相比没有违和感。
# 11: 20220530
### Title: Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study
### Venue: PMLR 2017
作者尝试从认知心理学的角度去探究深度神经网络的bias。本文探测的是shape bias，文章通过实验观察到：相比于Color， one shot learning models更倾向于通过shape去判断object的类别。
# 12: 20220530
### Title: Deep Residual Learning for Image Recognition
### Venue: CVPR 2016
作者提出了一个非常简单的深度卷积神经网络结构（残差链接），带来了非常大的效果提升。一定程度上解决了模型的效果随着网络深度增加而降低的问题。并且使得训练变得容易。深层原因：1. 因为残差结构，梯度变得打了，使SGD下降得更快也更有目标。2. 这种结构使得网络可以实现层数增加，但增加的层如果必要，可以对输入不做改变，一定程度上缓解了过拟合。
# 13: 20220531
### Title: Attention Is All You Need
### Venue: NIPS 2017
本文提出了一个新的神经网络架构：transformer。它没有用到卷积神经网络和循环神经网络，只用到了注意力，在翻译任务上取得好的效果。transformer现在已经应用在多种任务上，包括自然语言处理和计算机视觉。
# 14: 20220531
### Title: ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models
### Venue: ICCV 2021
作者使用训练好的diffusion model，在reverse过程加入了matching the latent variable of a given reference image，使得生成的结果与the given reference image有语义相似性，通过这种方式来控制diffusion model的生成结果。
# 15: 20220601
### Title: A Gentle Introduction to Graph Neural Networks
### Venue: Distill 2021
这篇文章发表在Distill上面，主要介绍了Graph Neural Networks，包括以下内容：什么是Graph，如何把data表示为Graph，GNN如何处理data等。GNN和CNN是有一些相似的地方，比如pooling操作。文章有许多生动形象、可交互的图片。
# 16: 20220602
### Title: Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization
### Venue: ICCV 2017
推荐代码仓库：https://github.com/jacobgil/pytorch-grad-cam. 本文的工作对于理解神经网络有很大的意义。本文工作在不需要改变神经网络结构也不需要重新训练的情况下，可视化出在某次任务中神经网络关注的区域在哪里，并且用热力图可视化出来。推荐代码仓库有许多类似方法在不同任务中的实现。
# 17: 20220603
### Title: Generative Adversarial Nets
### Venue: NIPS 2014
生成对抗网络（GAN）的开创者。生成器、鉴别器等这些耳熟能详的词，在2014年提出来。时至今日，GAN已经有了很多改进，也在实验性能上取得了非常惊人的效果。GAN在生成样本的diversity方面存在不足，在生成速度和生成质量方面有优势。
# 18: 20220604
### Title: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
### Venue: NAACL 2019
BERT对NLP任务的影响是巨大的。本文提出在语言类学习任务中，双向信息的重要性。在BERT上做微调效果很好，为下流任务提供了很好的效果提升。在本篇论文的结论中最大贡献是双向性（在写一篇论文的时候，最好有一个卖点，而不是这里好那里也好）。缺点是：与GPT（Improving Language Understanding by Generative Pre-Training）比，BERT用的是编码器，GPT用的是解码器。BERT做机器翻译、文本的摘要（生成类的任务）不好做。完整解决问题的思路：在一个很大的数据集上训练好一个很宽很深的模型，可以用在很多小的问题上，通过微调来全面提升小数据的性能（在计算机视觉领域用了很多年），模型越大，效果越好（很简单很暴力）。
# 19: 20220604
### Title: Text2Human: Text-Driven Controllable Human Image Generation
### Venue: SIGGRAPH 2022
本文的任务是生成包含human的image。在生成human穿的衣服的shape diversity和structure diversity上提出改进。本文的工作可以根据text生成对应样式的human image。生成human的衣服纹理这一块可以关注一下。采用的网络框架是VAE，构建了一个Hierarchical VQVAE。
# 20: 20220606
### Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
### Venue: ICLR 2021 Oral
作者用NLP任务中的BERT模型，来解决视觉任务（分类），提出了ViT: Vision Transformer。ViT和BERT模型的思路、结构等都是几乎一样的，不同的是输入由word变为了image patch。作者通过实验得出结论：ViT需要在大规模的数据集上才能取得超越CNN的效果。这个工作打破了CV和NLP之间的鸿沟，使得用一个model同时处理word和image成为显示，也就是处理多模态数据的model。
# 21: 20220607
### Title: Momentum Contrast for Unsupervised Visual Representation Learning
### Venue: CVPR 2020
一作Kaiming He，作者提出MoCo模型来学习特征，这些特征是通过无监督的对比学习获得的，学习到的特征可以迁移到下游任务中，比如分类和检测，效果可以媲美有监督学习的结果。该工作的官方代码写得非常好。
# 21: 20220616
### Title: Competition-Level Code Generation with AlphaCode
### Venue: arxiv 202202
作者团队DeepMind，提出AlphaCode，可以自动写代码的模型。作者用该模型去做算法竞赛的题，实验表明可以打败一般的人类算法程序员。用的架构是encoder-decoder，用的模型是transfermer。
