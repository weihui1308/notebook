# 20221001
## "人工智能安全与隐私"系列第九期:AI安全风险分析与测试修复-西交沈超教授
1. AI技术面临安全隐患：AI合成奥巴马讲话（深度伪造）、Facebook干预大选、虚假语音、自动驾驶系统事故、种族偏见等。
2. 传感器欺骗
3. 数据预处理阶段的攻击
# 20221004
## AI安全与隐私论坛第十期: UIUC李博教授-基于逻辑推理的可信赖机器学习
1. deep learning in the physical world
2. lidar和radar下的攻击
3. reasoning model
4. PrimateNet
5. the shapley value
6. generalization
7. 逻辑和deep learning的结合
8. transformer的robustness
9. 如何衡量物理攻击的隐蔽性（stealthiness）：心理学；主观；定义一些规则
10. 鲁棒性和泛化性的关系
# 20221006
## AI安全与隐私论坛第11期-清华大学崔鹏教授-稳定学习：寻找因果推理和机器学习之间的共同点
1. risk-sensitive areas
2. Problems of today's ML: Explainability、Stability(OOD Generalization Problem)、Fairness
3. causal inference
4. Trustworthy AI
5. 因果推理对AI Explainability的研究
# 20221007
## AI安全与隐私论坛第12期-密歇根州立大学汤继良教授-可信人工智能中的鲁棒性和公平性可以兼得吗
1. Trustworthy AI: to be Robust or to be Fair. 
2. Paper: To be Robust or to be Fair: Towards Fairness in Adversarial Training
3. Fairness: 性别；种族；人脸识别
4. Robustness和Fairness的关系：  
① 系统的安全性是由最短板决定的（木桶效应）
② improving adversarial robustness can cause fairness issues.
③ Fairness training can make AI models more vulnerable.
5. 人体的免疫系统
6. Paper: Trustworthy AI: A Computational Perspective.
# 20221008
## AI安全与隐私论坛第13期-复旦教授与腾讯专家-AI安全和模型版权保护
1. AI模型版权保护
2. Papers:  
① Stealing Machine Learning Models via Prediction APIs  
② Practical Black-Box Attacks against Machine Learning  
③ Knockoff Nets: Stealing Functionality of Black-Box Models  
④ MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation  
⑤ Embedding Watermarks into Deep Neural Networks
⑥ Protecting Intellectual Property of Deep Neural Networks with Watermarking  
⑥ IPGuard: Protecting Intellectual Property of Deep Neural Networks via Fingerprinting the Classification Boundary  
⑦ Deep Neural Network Fingerprinting by Conferrable Adversarial Examples  
⑧ ES Attack: Model Stealing against Deep Neural Networks without Data Hurdles
3. 采用backdoor attack进行模型的版权保护
4. 图像视频的篡改检测，吴祖煊，复旦大学
5. PyDeepFakeDet
# 20221008
## AI安全与隐私论坛第14期-浙江大学纪守领教授-针对特征归因算法的鲁棒性评估框架
1. AI Security: AI's Security and AI for Security
2. Adversarial Attack and Defence
3. paper: SirenAttack: Generating Adversarial Audio for End-to-End Acoustic Systems
4. paper: TextBugger: Generating Adversarial Text Against Real-world Applications
5. paper: Transfer Attacks Revisited: A Large-Scale Empirical Study in Real Computer Vision Settings
6. 活体检测安全性分析
7. 同时攻击分类和CAM
8. 特征归因算法
9. 从系统层面考虑对抗攻防（安全性问题）：整个系统不光是模型，还有别的环节
# 20221010
## AI安全与隐私论坛第15期:杜克大学Neil Gong-AI模型及衍生品的版权交易与保护
1. Machine Learning Pipeline
2. Stealing Attack(training data, model, weight, parameter)
3. paper: Stealing Hyperparameters in Machine Learning
4. paper: Stealing Links from Graph Neural Networks
5. paper: StolenEncoder: Stealing Pre-trained Encoders in Self-supervised Learning
6. Fingerprinting Models
7. paper: IPGuard: Protecting Intellectual Property of Deep Neural Networks via Fingerprinting the Classification Boundary
8. EncoderMI: Membership Inference against Pre-trained Encoders in Contrastive Learning
6. 腾讯AI大模型：刘威
7. 生成性AI的机遇与挑战：版权保护，安全（虚假信息等）
8. AI版权问题：传统的版权（模型、数据等）；生成内容的版权
# 20221010
## AI安全与隐私论坛第16期:天津大学韩亚洪教授-视觉深度模型的对抗鲁棒性与黑盒攻击
1. Decision-based Attack
2. paper: Query-efficient Black-box Adversarial Attack with Customized Iteration and Sampling
3. paper: Towards Transferable Adversarial Attacks on Vision Transformers
4. paper: Decision-based Black-box Attack Against Vision Transformers via Patch-wise Adversarial Removal
5. 联邦学习
6. paper: Machine behaviour
7. paper: Boosting Black-Box Attack with Partially Transferred Conditional Adversarial Distribution
# 20221021
## 报告题目：如何写好一个Rebuttal
1. [video 1](https://www.bilibili.com/video/BV1Ur4y1e7eX/?vd_source=6a269b59959fce1049e36a1aac54747c)
2. [video 2](https://www.bilibili.com/video/BV1R5411f7N2/?vd_source=6a269b59959fce1049e36a1aac54747c)
3. [video 3](https://www.bilibili.com/video/BV1944y1W7v1/?vd_source=6a269b59959fce1049e36a1aac54747c)
