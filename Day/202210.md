# 20221001
## "人工智能安全与隐私"系列第九期:AI安全风险分析与测试修复-西交沈超教授
1. AI技术面临安全隐患：AI合成奥巴马讲话（深度伪造）、Facebook干预大选、虚假语音、自动驾驶系统事故、种族偏见等。
2. 传感器欺骗
3. 数据预处理阶段的攻击
# 20221004
## AI安全与隐私论坛第十期: UIUC李博教授-基于逻辑推理的可信赖机器学习
1. deep learning in the physical world
2. lidar和radar下的攻击
3. reasoning model
4. PrimateNet
5. the shapley value
6. generalization
7. 逻辑和deep learning的结合
8. transformer的robustness
9. 如何衡量物理攻击的隐蔽性（stealthiness）：心理学；主观；定义一些规则
10. 鲁棒性和泛化性的关系
# 20221006
## AI安全与隐私论坛第11期-清华大学崔鹏教授-稳定学习：寻找因果推理和机器学习之间的共同点
1. risk-sensitive areas
2. Problems of today's ML: Explainability、Stability(OOD Generalization Problem)、Fairness
3. causal inference
4. Trustworthy AI
5. 因果推理对AI Explainability的研究
# 20221007
## AI安全与隐私论坛第12期-密歇根州立大学汤继良教授-可信人工智能中的鲁棒性和公平性可以兼得吗
1. Trustworthy AI: to be Robust or to be Fair. 
2. Paper: To be Robust or to be Fair: Towards Fairness in Adversarial Training
3. Fairness: 性别；种族；人脸识别
4. Robustness和Fairness的关系：  
① 系统的安全性是由最短板决定的（木桶效应）
② improving adversarial robustness can cause fairness issues.
③ Fairness training can make AI models more vulnerable.
5. 人体的免疫系统
6. Paper: Trustworthy AI: A Computational Perspective.
# 20221008
## AI安全与隐私论坛第13期-复旦教授与腾讯专家-AI安全和模型版权保护
1. AI模型版权保护
2. Papers:  
① Stealing Machine Learning Models via Prediction APIs  
② Practical Black-Box Attacks against Machine Learning  
③ Knockoff Nets: Stealing Functionality of Black-Box Models  
④ MAZE: Data-Free Model Stealing Attack Using Zeroth-Order Gradient Estimation  
⑤ Embedding Watermarks into Deep Neural Networks
⑥ Protecting Intellectual Property of Deep Neural Networks with Watermarking  
⑥ IPGuard: Protecting Intellectual Property of Deep Neural Networks via Fingerprinting the Classification Boundary  
⑦ Deep Neural Network Fingerprinting by Conferrable Adversarial Examples  
⑧ ES Attack: Model Stealing against Deep Neural Networks without Data Hurdles
3. 采用backdoor attack进行模型的版权保护
4. 图像视频的篡改检测，吴祖煊，复旦大学
5. PyDeepFakeDet
