1. Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity
2. WarpingGAN:Warping Multiple Uniform Priors for Adversarial 3D Point Cloud Generation
3. Cross-Modal Transferable Adversarial Attacks from Images to Videos
4. Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity
5. Adversarial Eigen Attack on Black-Box Models
6. Towards Efficient Data Free Black-box Adversarial Attack
7. Can You Spot the Chameleon? Adversarially Camouflaging Images from Co-Salient Object Detection
8. Robust Combination of Distributed Gradients Under Adversarial Perturbations
9. Shape-invariant 3D Adversarial Point Clouds
10. Self-supervised Learning of Adversarial Examples: Towards Good Generalizations for Deepfake Detection
11. Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon
12. Fairness-aware Adversarial Perturbation Towards Bias Mitigation for Deployed Deep Models
13. Boosting Black-Box Attack with Partially Transferred Conditional Adversarial Distribution
14. Improving the Transferability of Targeted Adversarial Examples through Object-Based Diverse Input
15. Exploring Frequency Adversarial Attacks for Face Forgery Detection
16. Improving Adversarial Transferability via Neuron Attribution-Based Attacks
17. Transferable Sparse Adversarial Attack
18. Segment and Complete: Defending Object Detectors against Adversarial Patch Attacks with Robust Patch Detection
19. Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability
20. Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart
21. Protecting Facial Privacy: Generating Adversarial Identity Masks via Style-robust Makeup Transfer
22. Localized Adversarial Domain Generalization
23. BppAttack: Stealthy and Efficient Trojan Attacks against Deep Neural Networks via Image Quantization and Contrastive Adversarial Learning
24. Bounded Adversarial Attack on Deep Content Features
25. Give Me Your Attention: Dot-Product Attention Considered Harmful for Adversarial Patch Robustness
26. Fingerprinting Deep Neural Networks Globally via Universal Adversarial Perturbations
27. DST: Dynamic Substitute Training for Data-free Black-box Attack
28. Quarantine: Sparsity Can Uncover the Trojan Attack Trigger for Free
29. Backdoor Attacks on Self-Supervised Learning
30. Exploring Effective Data for Surrogate Training Towards Black-box Attack
31. 360-Attack: Distortion-Aware Perturbations from Perspective-Views
32. FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis
33. Towards Practical Deployment-Stage Backdoor Attack on Deep Neural Networks
34. Bandits for Structure Perturbation-based Black-box Attacks to Graph Neural Networks with Theoretical Guarantees
35. Appearance and Structure Aware Robust Deep Visual Graph Matching: Attack, Defense and Beyond
36. DEFEAT: Deep Hidden Feature Backdoor Attacks by Imperceptible Perturbation and Latent Representation Constraints
37. DTA: Physical Camouflage Attacks using Differentiable Transformation Network
38. Zero-Query Transfer Attacks on Context-Aware Object Detectors
39. Label-Only Model Inversion Attacks via Boundary Repulsion
40. Investigating Top-$k$ White-Box and Transferable Black-box Attack